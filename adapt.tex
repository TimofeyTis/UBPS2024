
\section{Модуль адаптивного управления светофорными объектами}
\label{par:MARLIN2}

Опишем задачу управления светофорным объектом  как задачу управления агентом в стохастической среде.
Агент (светофорный объект) не располагает ресурсами и решает задачу целесообразности активации той или иной фазы. Обозначим множество всех действий агента символом $\action$. \linebreak Среда --- детектируемые перекрестки с  оптическими датчиками, которые распознают машины на отрезках дорог за сто метров до стоп-линий. Состояние среды отражает активность фаз светофорных объектов и время, которое машины находятся в детектируемой зоне. Обозначим множество всех состояний символом $\state$.

В качестве математической модели сети светофоров в работе рассматривается управляемый марковский процесс с конечным числом действий и  состояний. %$\left\langle  \state, \action, \mathbb{P}, r \right\rangle$ .
Таким образом, проблема управления светофорными объектами сводится к задаче мультиагентного обучения с подкреплением (Multiagent Reinforcement Learning).





\subsection{Задача мультиагентного обучения с подкреплением для светофорных объектов}
\label{subpar: MARL Q learning}
Опишем поведение светофорных объектов (агентов) с помощью марковского процесса принятия решений $\left\langle  \state, \action, \mathbb{P}, r \right\rangle$ \cite{SUTTON}.
Процесс принятия решений для агента будет выглядеть следующим образом. В момент времени $t$ агент наблюдает состояние  среды $s_t\in\state$ и выбирает действие $a_t \in\action$. Среда отвечает генерацией наград $R_t = r(s_t, a_t)$ и переходит в  состояние
$s_{t+1}=s'$ \linebreak с вероятностью $p(s' \bigm| s_t, a_t)$ согласно матрице переходов  $\mathbb{P}.$

  {
    Функция оценки эффективности применяемого управления $\delta= \{a_t, t \in \mathbb{N}\},$ составляющая траекторию процесса, \linebreak ${\cal T} = \left\{s_0,a_0, s_1,a_1, \dots, s_T,a_T \right\}$ получается как  функция:
    \abovedisplayskip = -3.5em
    \begin{equation}\label{VALUE}
      \begin{split}
        V & = \sum_{t=0}^{\infty}\gamma^t r\left( s_{t+1} \bigm| s_t,\delta_t\right) 
        = \lim\limits_{T\to\infty}\mathbb{E}_{{\cal T}} \sum_{t=0}^{T}\gamma^t R_t,
      \end{split}
    \end{equation}
  }\noindent
где  величина  $0 < \gamma < 1$, называется коэффициентом переоценки и показывает во сколько раз уменьшается отложенное вознаграждение за один временной шаг. 
Переоценка задает приоритет получения награды в ближайшее время перед получением той же награды через некоторое время. 
Математический смысл условия $0 < \gamma < 1$ состоит в том, 
чтобы гарантировать ограниченность функционала $V.$

Формальная постановка  задачи вычисления оценки эффективности управления светофорным объектом представлена ниже.
\begin{center}
  \begin{tabular}{rl}
    {\sf Дано}:
     &
    \parbox[t]{0.84\textwidth}{марковский процесс принятия решения $\left\langle  \state, \action, \mathbb{P}, r \right\rangle$ для  управления светофорным объектом, активная  в начальный момент времени фаза светофорного объекта $s_0$.}
    \\
    {\sf  Найти}:
     &
    \parbox[t]{0.84\textwidth}{
    управление  светофорного  объекта $\delta^*=\{ a^*_t\}_{0\leq t<\infty}$,\;\;\;\;  которое  доставит максимум функции оценки его\;\;\;\;\;\;\;\;  эффективности (\ref{VALUE}).
    }
  \end{tabular}
\end{center}

Решение задачи поиска  оптимального совокупного управления светофорными объектами дорожной сети ищется методом динамического программирования согласно принципу оптимальности Вальда---Беллмана.
% \begin{proposition1}\label{prep:kropt}
В задаче управления фазами светофорного объекта уравнение Вальда---Беллмана имеет вид \cite{My3}
  {
  \abovedisplayskip=0.5em
  \belowdisplayskip=0em
  \begin{equation}\label{BELMAN}
    V^* = \max_{a \in \action} \sum_{s'\in \state} p(s'\bigm|s, a)(r(s, a)+\gamma V^*(s')).
  \end{equation}}
  %для управляемого марковского процесса с конечным числом действий и состояний в контексте  задачи (\ref{VALUE}).
% \end{proposition1}

Формула (\ref{BELMAN}) может быть переписана в итерационной записи, называемой $Q$-обучением \cite{SUTTON}.
Функция суммарных вознаграждений при оптимальном управлении на шаге $t$ имеет вид
{\abovedisplayskip=0.5em
\belowdisplayskip=0em
\[
  V^*\left(\{s_{t'}, \delta \}_{t'\in \mathbb{N},\;t'\leqslant t}\right)=\max\limits_{a\in{\cal \action}} Q_t(s_t, a).
\]}

Считаем, что нам известно состояние среды $s_{t+1}$ и оптимальное управление $a_{t+1} $ на шаге $t+1,$ соответствующий итерации ${l},$ и условимся, что итерация $Q$ идет по индексу $l$, тогда
функция $Q$ для агента представима в рекурсивном виде 
\abovedisplayskip=0em
\belowdisplayskip=0.0em
\begin{equation*}
  \begin{split}
    Q_{l+1}(s,a) & = 
           \underbrace{p(s_{t+1}| s,a)\rule[-0.8em]{0em}{2em}}_{\alpha_{l}} \Bigl( r_{t+1} + \gamma V^*({s}_{t+1})\Bigr)  +                                                                         \\
                                     & + \underbrace{\sum\limits_{{s}' \in \state /{s}_{t+1}} p({s}'| {s},{a}) }_{1-\alpha_{l}}\Bigl( r({s}'| {s},{a}) + \gamma V({s}') \Bigr) = \\
                                     & =\alpha_{l}\Bigl( r_{l} + \gamma\max\limits_{\textcolor{red}{a}'\in \action}Q_{l}({s}_{t+1},\textcolor{red}{a}')\Bigr)  + \Bigl(1- \alpha_{l}\Bigr) Q_{l}({s},{a}).
  \end{split}
\end{equation*}

Цель поиска  оптимального совокупного управления светофорными объектами дорожной сети заключается в  увеличении максимального совокупного вознаграждения, определяемого функцией $Q.$ 
% \begin{proposition1}\label{prep:search}
   Для задачи поиска оптимального управления светофорным объектом справедливы следующие утверждения \cite{My3}:
  % \begin{equation}%\label{VALUE}
  %V(s) = \mathbb{E}\sum_{t=0}^{\infty}\gamma^t r(s_t,\delta(s_t)) \rightarrow\max\limits_\delta, s_0 = s
  % \end{equation}
  \begin{itemize}[itemsep=-3pt,topsep=0pt]
    \item существует единственное точное решение;
    \item оценка точности приближенного  решения на $n$-ом шаге имеет вид
          {
          $
            \rho(Q_n, Q_0) \leqslant \frac{\displaystyle \gamma ^n \rho(Q_1, Q_0)}{\displaystyle  1 - \gamma},
          $}\\
          \noindent
          где $Q_l \in \mathbb{R} _{\infty}^{| \action|+| \state|}$ --- вектора значений $Q(s, a)$ на шаге $l$,\\
          $\forall q,w \in \mathbb{R} _{\infty}^{| \action|+| \state|}$   расстояние 
          $\rho(q, w) = \max\limits_{ {\scriptstyle j\in \mathbb{N}, \rule{0em}{.9em}}\;{\mathmakebox[1em][l]{\scriptstyle j\leqslant {\left|\action\right|+\left|\state\right|}} }}\left| {q}_j - {w}_j\right|;$ % --- метрика пространства  $\mathbb{R} _{\infty}^{| \action|+| \state|}$, определенная для $\forall q,w \in \mathbb{R} _{\infty}^{| \action|+| \state|};$
    \item приближенное решение находится согласно формулам
          \begin{equation}\label{Voptsolve}
            V ^*(s) = \max_{a\in  \action}  \lim _{l \to + \infty}Q_l(s, a),     \end{equation}
          \begin{equation}\label{argoptsolve}
            a_t(s) = \arg  \max_{a' \in  \action} Q_l(s, a').  \end{equation}



          %$\left\langle {\mathbb{R} _{\infty}^{| \action|+| \state|},\rho(\cdot, \cdot)} \right\rangle$ --- полное метрическое пространство c 
          % $\forall q, w \in \mathbb{R} _{\infty}^{| \action|+| \state|}$ 
          %метрикой $\rho(q, w) = \max\limits_{ 1\leqslant j \leqslant \left|A\right|+\left|S\right|} \left| {q}_j - {w}_j\right|,$ $\forall q, w \in \mathbb{R} _{\infty}^{| \action|+| \state|}.$


  \end{itemize}
  %Оценка точности приближенного  решения на $n$-ом шаге итерации имеет вид
  %  $$
  % \rho(Q_n(s, a),Q_0(s, a)) \leqslant \frac{\gamma ^n \rho(Q_1(s, a),Q_0(s, a))}{ 1 - \gamma}.
  % $$
% \end{proposition1}



\subsection{Описание модуля}
\label{subpar: MARLIN24_descri}

Схема  подсчета функции оценки эффективности управления управления светофорными объектами
представлена на рисунке~\ref{fig:MARLIN_alg}.
\begin{figure}[tbph]
  \centering
  \includegraphics[width = 0.7\textwidth]{mod_scheme.pdf}
  \caption{\textcolor{red}{Схема алгоритма управления светофорными объектами в комплексе MARLIN24 }}\label{fig:MARLIN_alg}
\end{figure}
\textcolor{red}{}


В имитационной среде (SIMULATION) моделируется транспортные потоки с интенсивностями, полученными в модуле валидации, автомобили перемещаются в имитационной среде (SIMULATION) пока  не выйдут из ее зоны покрытия. 
При попадании машины на детектируемый участок дорожной сети $z,$ во вспомогательном модуле,  имитирующем поступление информации с оптических датчиков (VEHICLE DETECTOR), пары, состоящие из указателей на объект машины и текущего времени модели, добавляются в одну из коллекций tcf (time collection forward) 
для выбранного вручную множества светофорных объектов TrafficLight1, \dots, TrafficLightK. 
На следующем шаге симуляции машины удаляются из коллекции tcf, при проезде через зону. 
В течении периода времени $dt$ во вспомогательном модуле выбора управления вызывается процесс переключения фаз (QLEARNING),
% зачем ссылка на учебники??? \cite{Ivanov,Gasnikov} 
реализующий управление согласно выбранной стратегии совокупного управления. 

В качестве альтернативы процессу переключения фаз светофора  (QLEARNING), использующемуся по умолчанию в  модуле адаптивного  управления светофорными объектами, можно  выбрать изменение длительности фазы в следующем цикле. Данный подход был успешно реализован с помощью контроллера нечеткой логики (FLCONTROL) в работе \cite{My3}. 
% Для управления светофором на основе метода Мамдани разработана система типа MISO (Multiple Input Single Output) с тремя входами \cite{refarticle24}. 

% Следующие лингвистические переменные рассматриваются в качестве входных данных:
% время суток с множеством термов ${\cal{DT}} = \{\text{morning}, \text{day}, \text{evening}, \text{night}\},$
% плотность движения с множеством термов ${\cal V} =  \{ \text{run}, \text{wait}, \text{jam}\},$
% время нахождения в зоне детекции с множеством термов \linebreak ${\cal R} =  \{\text{small}, \text{medium}, \text{large}\}.$
% Лингвистическая переменная выхода контроллера --- длительность фазы, определенная множеством термов  $P=$ 
% \{ {long}, {medium}, {short}\}.  
% База правил представлена двенадцатью правилами (таблица~\ref{table:FL_rules}) следующего вида:
% \begin{equation*}
% \begin{split}
%     R:  &{\rm IF}\  (day\_time = A)\ {\rm AND}\  (vehicles = B)\  {\rm AND}\  (reward = C) \\
%     &{\rm THEN}\  (phase = D),\;\rm{WHERE}\; A \in {\cal DT}, B \in {\cal V}, C \in {\cal R}, D \in {\cal P}.
% \end{split}
% \end{equation*}


% \begin{table}[h]
%   \caption{Набор правил нечеткого вывода}
%   \label{table:FL_rules}
%   \centering
%   \scalebox{0.75}
%   {
%   \begin{tabular}{c|c|c|c|c|c|c|}
%   \cline{2-7}
%                                   & $R_1$ & $R_2$ & $R_3$ & $R_4$ & $R_5$ & $R_6$  \\ \hline
%   \multicolumn{1}{|c|}{$day\_time$} & morning  & night  & -  & -  & day  & day  \\ \hline
%   \multicolumn{1}{|c|}{$vehicles$}  & -  & -  & -  & run  & jam  & -   \\ \hline
%   \multicolumn{1}{|c|}{$reward$}    & -  & -  & small  & -  &medium & medium  \\ \hline
%   \multicolumn{1}{|c|}{$phase$}     & short  & short  & short  & short & short  & medium   \\ \hline
%   \end{tabular}
%   }

%   \medskip
  
%   \scalebox{0.75}
%   {
%   \begin{tabular}{c|c|c|c|c|c|c|}
%   \cline{2-7}
%                                  &$R_7$ & $R_8$ & $R_9$ & $R_{10}$ & $R_{11}$ & $R_{12}$ \\ \hline
%   \multicolumn{1}{|c|}{$day\_time$} & evening &  day   & evening  & -   & -   & -   \\ \hline
%   \multicolumn{1}{|c|}{$vehicles$}  &  -  & -  & -  & wait   & jam   & jam   \\ \hline
%   \multicolumn{1}{|c|}{$reward$}    &  medium  & large  & large  & medium   & large   & small   \\ \hline
%   \multicolumn{1}{|c|}{$phase$}     &  medium  & long  & long  & long   & long   & short   \\ \hline
%   \end{tabular}
%   }
%   \end{table}

% На основе выходных данных модуля принимается решение о переключении фазы светофоров или об изменении длительности следующей фазы. % (см. рис. \ref{MARLIN_mod}).