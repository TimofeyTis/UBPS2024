
\section{Модуль адаптивного управления светофорными объектами}
\label{par:MARLIN2}

Опишем задачу управления светофорным объектом  как задачу управления агентом в стохастической среде.
Агент (светофорный объект) не располагает ресурсами и решает задачу целесообразности активации той или иной фазы. Обозначим множество всех действий агента символом $\action$. \linebreak Среда --- детектируемые перекрестки с  оптическими датчиками, которые распознают машины на отрезках дорог за сто метров до стоп-линий. Состояние среды отражает активность фаз светофорных объектов и время, которое машины находятся в детектируемой зоне. Обозначим множество всех состояний символом $\state$.

В качестве математической модели сети светофоров в работе рассматривается управляемый марковский процесс с конечным числом действий и  состояний. %$\left\langle  \state, \action, \mathbb{P}, r \right\rangle$ .
Таким образом, проблема управления светофорными объектами сводится к задаче мультиагентного обучения с подкреплением (Multiagent Reinforcement Learning).





\subsection{Задача мультиагентного обучения с подкреплением для светофорных объектов}

Опишем поведение светофорных объектов (агентов) с помощью марковского процесса принятия решений $\left\langle  \state, \action, \mathbb{P}, r \right\rangle$ \cite{sutton2015}.
Процесс принятия решений для агента будет выглядеть следующим образом. В момент времени $t$ агент наблюдает состояние среды  $s_t\in\state$ и выбирает действие $a_t \in\action$. Среда отвечает генерацией награды $R_t = r(s_t, a_t)$ и переходит в следующее состояние
$s_{t+1}=s'$ с вероятностью $p(s' \bigm| s_t, a_t)$ по матрице переходов  $\mathbb{P}.$

  {
    Функция оценки эффективности применяемого управления $\delta= \{a_t, t \in \mathbb{N}\}$ составляющая траекторию процесса \linebreak ${\cal T} = \left\{s_0,a_0, s_1,a_1, \dots, s_T,a_T \right\}$ получается как  функция:
    \abovedisplayskip = -3.5em
    \begin{equation}\label{VALUE}
      \begin{split}
        V & = \sum_{t=0}^{\infty}\gamma^t r\left( s_{t+1} \bigm| s_t,\delta_t\right) 
        = \lim\limits_{T\to\infty}\mathbb{E}_{{\cal T}} \sum_{t=0}^{T}\gamma^t R_t,
      \end{split}
    \end{equation}
  }\noindent
где  величина $\gamma$, $0 < \gamma < 1$, называется коэффициентом переоценки и показывает во сколько раз уменьшается отложенное вознаграждение за один временной шаг. 
Переоценка задает приоритет получения награды в ближайшее время перед получением той же награды через некоторое время. 
Математический смысл условия $0 < \gamma < 1$ состоит в том, 
чтобы гарантировать ограниченность функционала $V.$

Формальная постановка  задачи вычисления оценки эффективности управления светофорным объектом представлена ниже.
% \begin{center}
  \begin{tabular}{rl}
    {\sf Дано}:
     &
    \parbox[t]{0.84\textwidth}{марковский процесс принятия решения $\left\langle  \state, \action, \mathbb{P}, r \right\rangle$ для  управления светофорным объектом, активная  в начальный момент времени фаза светофорного объекта $s_0$.}
    \\
    {\sf  Найти}:
     &
    \parbox[t]{0.84\textwidth}{
    управление  светофорного  объекта $\delta^*=\{ a^*_t\}_{0\leq t<\infty}$,\;\;\;\;  которое  доставит максимум функции оценки его\;\;\;\;\;\;\;\;  эффективности (\ref{VALUE}).
    }
  \end{tabular}
% \end{center}

Решение задачи поиска  оптимального совокупного управления светофорными объектами дорожной сети ищется методом динамического программирования согласно принципу оптимальности Вальда---Беллмана. Cправедливо слудующее утверждение.

\begin{proposition1}\label{prep:kropt}
  \cite{tislenko2022b} В задаче управления фазами светофорного объекта уравнение Вальда---Беллмана имеет вид
  {
  \abovedisplayskip=-0.0em
  \belowdisplayskip=0em
  \begin{equation}\label{BELMAN}
    V^* = \max_{a \in \action} \sum_{s'\in \state} p(s'\bigm|s, a)(r(s, a)+\gamma V^*(s')).
  \end{equation}}
  %для управляемого марковского процесса с конечным числом действий и состояний в контексте  задачи (\ref{VALUE}).
\end{proposition1}

Формула (\ref{BELMAN}) может быть переписана в итерационной записи, называемой $Q$-обучение.
Функция суммарных вознаграждений при оптимальном управлении на шаге $t$ имеет вид
{\abovedisplayskip=0.5em
\belowdisplayskip=0.0em
\[
  V^*\left(\{s_{t'}, \delta \}_{t'\in \mathbb{N},\;t'\leqslant t}\right)=\max\limits_{a\in{\cal \action}} Q_t(s_t, a),
\]}

Считаем, что нам известно состояние среды $s_{t+1}$ и оптимальное управление $a_{t+1} $ на шаге $t+1,$ соответствующий итерации ${l},$ и условимся, что итерация $Q$ идет по индексу $l$, тогда
функция $Q$ для агента имеет рекурсивную запись 
\abovedisplayskip=0.5em
\belowdisplayskip=0.0em
\begin{equation*}
  \begin{split}
    Q_{l+1}(s,a) & = 
           \underbrace{p(s_{t+1}| s,a)\rule[-0.8em]{0em}{2em}}_{\alpha_{l}} \Bigl( r_{t+1} + \gamma V^*({s}_{t+1})\Bigr)  +                                                                         \\
                                     & + \underbrace{\sum\limits_{{s}' \in S/{s}_{t+1}} p({s}'| {s},{a}) }_{1-\alpha_{l}}\Bigl( r({s}'| {s},{a}) + \gamma V({s}') \Bigr) = \\
                                     & =\alpha_{l}\Bigl( r_{l} + \gamma\max\limits_{{s}'}Q_{l}({s}_{t+1},{s}')\Bigr)  + \Bigl(1- \alpha_{l}\Bigr) Q_{l}({s},{a}).
  \end{split}
\end{equation*}

Решение задачи поиска  оптимального совокупного управления светофорными объектами дорожной сети ищется, чтобы увеличить максимальное совокупное вознаграждение, определяемое функцией $Q.$ 
Cправедливо слудующее утверждение.
\begin{proposition1}\label{prep:search}
  \cite{tislenko2022b} Для задачи поиска оптимального управления светофорным объектом с любым количеством фаз
  % \begin{equation}%\label{VALUE}
  %V(s) = \mathbb{E}\sum_{t=0}^{\infty}\gamma^t r(s_t,\delta(s_t)) \rightarrow\max\limits_\delta, s_0 = s
  % \end{equation}
  \begin{itemize}[itemsep=-3pt]
    \item существует единственное точное решение;
    \item оценка точности приближенного  решения на $n$-ом шаге итерации
          {
          \belowdisplayskip=0em
          $$
            \rho(Q_n, Q_0) \leqslant \frac{\gamma ^n \rho(Q_1, Q_0)}{ 1 - \gamma},
          $$}
          \noindent 
          где $Q_l \in \mathbb{R} _{\infty}^{| \action|+| \state|}$ --- вектора значений $Q(s, a)$ на шаге $l$,\\
          $\forall q,w \in \mathbb{R} _{\infty}^{| \action|+| \state|}$   расстояние 
          $\rho(q, w) = \max\limits_{ {\scriptstyle j\in \mathbb{N}, \rule{0em}{.9em}}\;{\mathmakebox[1em][l]{\scriptstyle j\leqslant {\left|\action\right|+\left|\state\right|}} }}\left| {q}_j - {w}_j\right|;$ % --- метрика пространства  $\mathbb{R} _{\infty}^{| \action|+| \state|}$, определенная для $\forall q,w \in \mathbb{R} _{\infty}^{| \action|+| \state|};$
    \item приближенное решение находится согласно формулам
          \begin{equation}\label{Voptsolve}
            V ^*(s) = \max_{a\in  \action}  \lim _{l \to + \infty}Q_l(s, a),     \end{equation}
          \begin{equation}\label{argoptsolve}
            a_t(s) = \arg  \max_{a' \in  \action} Q_l(s, a').  \end{equation}



          %$\left\langle {\mathbb{R} _{\infty}^{| \action|+| \state|},\rho(\cdot, \cdot)} \right\rangle$ --- полное метрическое пространство c 
          % $\forall q, w \in \mathbb{R} _{\infty}^{| \action|+| \state|}$ 
          %метрикой $\rho(q, w) = \max\limits_{ 1\leqslant j \leqslant \left|A\right|+\left|S\right|} \left| {q}_j - {w}_j\right|,$ $\forall q, w \in \mathbb{R} _{\infty}^{| \action|+| \state|}.$


  \end{itemize}
  %Оценка точности приближенного  решения на $n$-ом шаге итерации имеет вид
  %  $$
  % \rho(Q_n(s, a),Q_0(s, a)) \leqslant \frac{\gamma ^n \rho(Q_1(s, a),Q_0(s, a))}{ 1 - \gamma}.
  % $$
\end{proposition1}



\subsection{Описание модуля}


Схема  подсчета функции оценки эффективности управления управления светофорными объектами
представлена на рисунке~\ref{fig:MARLIN_alg}.
\begin{figure}[tbph]
  \centering
  \includegraphics[width = 0.7\textwidth]{mod_scheme.pdf}
  \caption{Схема алгоритма MARLIN24 }\label{fig:MARLIN_alg}
\end{figure}
\textcolor{red}{}


В имитационной среде (SIMULATION) моделируется транспортные потоки с интенсивностями, полученными в модуле валидации, автомобили перемещаются в имитационной среде (SIMULATION) пока  не выйдут из ее зоны покрытия. При попадании машины на детектируемый участок дорожной сети $z,$ во вспомогательном модуле,  имитирующем поступление информации с оптических датчиков (VEHICLE DETECTOR), пары, состоящие из указателей на объект машины и текущего времени модели, добавляются в одну из коллекций tcf (time collection forward) 
для выбранного вручную множества светофорных объектов TrafficLight1, \dots, TrafficLightK. 
На следующем шаге симуляции машины удаляются из коллекции tcf, при проезде через зону. 
В течении периода времени $dt$ во вспомогательном модуле выбора управления вызывается модуль QLEARNING,
% зачем ссылка на учебники??? \cite{Ivanov,Gasnikov} 
реализующий управление согласно выбранной стратегии совокупного управления. На основе выходных данных модуля принимается решение о переключении фазы светофоров. % (см. рис. \ref{MARLIN_mod}).


\section{Структура программного комплекса}

Опишем подробнее процесс подсчета $\hat{\pi}(a| s),$ опираясь на структурную схему.
Модуль адаптивного управления светофорными объектами загружает управляющий конфигурационный файл trafficLightConfig.xml. В конфигурационном файле содержится информация о возможных направлениях движения, количестве фаз и циклах светофорных объектов.
Далее  комплекс программных средств MARLIN24  связывает  показания  датчика в имитационном модуле  и рассчитывает оптимальное управление для светофорных объектов.

Пусть оптические датчики (VEHICLE DETECTOR) в имитационной среде (SIMULATION)  записывают момент появления $t_i$ пронумерованного транспортного средства $i \in I \subset \mathbb{N}$ в  зоне $z\in Zones  = \left\{z^{(0)}, z^{(1)}, \dots, z^{(m)} \right\}, m \in \mathbb{N}$.
Отметим, что при имитационном моделировании   псевдослучайная интенсивность движения транспортных средств будет задана алгоритмически. Это означает, что мы можем сконструировать множество пар $(i,z),$ что  автомобиль $i$ находится в детектируемой зоне $z$ в момент времени $t.$ Определим данное множество как отношение $\psi_t \subset \mathbb{N} \times Zones, $ для которого  $i\psi_t z.$
Введем также отношение $\phi \subset Zones \times \state,$ описывающее зоны $z, $ в которых состояние $s'$ разрешает движение.
Сгруппируем автомобили в  зонах в соответствии с фазой светофорного объекта $s'$, которая разрешает движение транспортных средств в этих зонах и обозначим
$
  I(s',t) = \left\{i|\;\; t_i < t, i \psi_t z, z \phi s' \right\}.
$

Приведем рассуждения, исходя из которых считается функция вознаграждения.
Для каждой полосы определено число машин  на отрезке дороги, начинающемся с детектора и заканчивающемся стоп-линией перекрестка.
Пусть  $r:  \state \times \action \mapsto \mathbb{R}$ --- функция вознаграждения агента при изменении наблюдаемого состояния $s_t$ при действии $a_t=\delta(s)$. 
В момент времени $t$ значение функции $r(s_t, a_t)=R_t$  определяется для следующей активной полосы и пропорционально времени, затраченному всеми машинами на преодоление детектируемых участков дороги $R_t = \sum\limits_{\mathmakebox[1em][l]{i \in I(s',t)}} (t -t_i).$

Далее для построенного множества светофорных объектов $TL$  и зон детекции $z$ определяются функция наград $r(s,a)$ число проехавших машин (MCOUNT), суммарное время проезда через детектируемые участки дорожной сети (TIME\_SUMM) и  обучающие функции $Q.$
При симуляции, формируется
двумерная выборка  ${\cal X}=\left\{(s_i, a_i)\right\}_{i=1}^T$
объемом $T$ порядка $10^6$. В результате  управления $\delta^*,$ принятого из соображений увеличения значения функции оценки эффективности $Q,$  рассчитывается  несмещенная оценка
распределения ${\cal P}=\{p(s, a)\}_{s\in{\state}, a\in{\action}}$ двумерной случайной величины $(s, a)$, где функция распределения
$p(s, a)$  --- вероятность того, что в состоянии $s$ агент принял решение $a$.
На основании
выборочных вероятностей  $\hat{p}(s, a)$ вычисляются оценки
политики агента $\hat{\pi}(a| s)$  для каждого $s\in\state$
$$
  \hat{\pi}(a| s)=\frac{\hat{p}(s, a)}{\displaystyle \sum\limits_{a\in{\mathbb A}}\hat{p}(s,a)}=\frac{\hat{p}(s, a)}{\hat{p}(s)}.
$$
%На практике при решении прямой задачи распределение $\perprob\left(s , a\right)$ неизвестно, но т.к. исследуется выборка объемом порядка $10^6$, то по ней возможно восстановить функцию распределения случайной величины. Согласно закону больших чисел
% (ссылка на справочник большакова  
%оно находится с помощью известной  эмпирической функции распределения $\perprob^*_n\left(s , a\right).$
Наряду с политиками агента, при обработке интенсивностей записываются массивы  $r_{a^{(k)}} =\left\{r(s_0,a^{(k)}),r(s_1,a^{(k)}),r(s_2,a^{(k)}),\dots\right\}$, $ k\in K$.
Элементы массивов $r\left(s_t,a^{(k)}\right)$  --- время нахождения машин на активируемых  агентом $k$ и фазой  $a^{(k)}\oplus s_t$  полосах. %Отсчет времени начинается с момента обнаружения машины датчиками.
% Результат обработки интенсивности трафика представлен на рисунке \ref{fig:intens1}.
% \red{добавить рисунок}
% \begin{figure}[H]
%     \centering
%     %\includegraphics[width=0.99\textwidth]{exintens.png}
%     \caption{Интенсивность трафика на перекрестке в течение 24 часов, зеленый цвет соответствует фазе $s^{(0)}$, красный --- $s^{(1)}$}
%     \label{fig:intens1}
% \end{figure}